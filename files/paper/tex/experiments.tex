\section{Experiments}
\subsection{Datasets}
We used several subsets of Imagenet~\cite{deng2009imagenet} for training and evaluation of our methods. We also evaluate a 
tracking
algorithm on a video of scuba divers taken from Youtube \footnote{https://www.youtube.com/watch?v=QmRFmhILd5o}. Subsets of 
Imagenet
containing underwater images were selected for the training of CycleGAN, and manually separated into two classes based on visual
inspection. We let $X$ be the set of underwater images with no distortion, and $Y$ be the set of underwater images with 
distortion. $X$
contained 6143 images, and $Y$ contained 1817 images. We then trained CycleGAN to learn the mapping $F: X \rightarrow Y$, such 
that images
from $X$ appeared to have come from $Y$. Finally, our image pairs for training data were generated by distorting all images in $X$ 
with
$F$. Figure~\ref{fig:cgan_samples} shows sample training pairs. When comparing with CycleGAN, we used a test set of 56 images 
acquired from
Flickr\texttrademark .

\subsection{Evaluation}
We train UGAN and UGAN-P on the image pairs generated by CycleGAN, and evaluate on the images from the
test set, $Y$. Note that these images do not contain any ground truth, as they are original distorted images from
Imagenet. Images for training and testing are of size $256 \times 256 \times 3$ and normalized between $[-1, 1]$.
Figure ~\ref{fig:test_samples} shows samples from the test set. Notably, these images contain varying amounts of noise. Both UGAN 
and UGAN-P
are able to recover lost color information, as well as correct any color information this is present. 

While many of the distorted images contain a blue or green hue over the entire image space, that is not always the case.
In certain environments,
it is possible that objects close to the camera are undistorted with correct colors, while the background
of the image contains distortion. In these cases, we would like the network to only correct parts of the image that
appear distorted. The last row in Figure ~\ref{fig:test_samples} shows a sample of such an image. The orange of the clownfish is 
left
unchanged while the distorted sea anemone in the background has its color corrected.

We use three metrics for a quantitative evaluation. First, we compare to CycleGAN, as it inherently learns an inverse
mapping during the training of $F: X \rightarrow Y$, using the Canny edge detector
\cite{canny1986computational}. This provides a color agnostic evaluation of the images in comparison to ground truth.
Second, we compare local image patches to provide sharpness metrics on our images. Lastly, we show how an existing
tracking algorithm for an underwater robot improves performance with generated images.

\begin{figure}
\centering
\begin{tabular}{p{1.7cm} p{1.7cm} p{1.7cm}}

   ~\quad Original & ~\quad UGAN & \quad UGAN-P \\

   \includegraphics[width=0.8in]{n01917289_4982_real} &
   \includegraphics[width=0.8in]{n01917289_4982_gen_0} &
   \includegraphics[width=0.8in]{n01917289_4982_gen_1} \\

   \includegraphics[width=0.8in]{n01496331_16340_real} &
   \includegraphics[width=0.8in]{n01496331_16340_gen_0} &
   \includegraphics[width=0.8in]{n01496331_16340_gen_1} \\
   
   \includegraphics[width=0.8in]{n01496331_22183_real} &
   \includegraphics[width=0.8in]{n01496331_22183_gen_0} &
   \includegraphics[width=0.8in]{n01496331_22183_gen_1} \\
   
   \includegraphics[width=0.8in]{n01664065_29738_real} &
   \includegraphics[width=0.8in]{n01664065_29738_gen_0} &
   \includegraphics[width=0.8in]{n01664065_29738_gen_1} \\
   
   \includegraphics[width=0.8in]{n01496331_11938_real} &
   \includegraphics[width=0.8in]{n01496331_11938_gen_0} &
   \includegraphics[width=0.8in]{n01496331_11938_gen_1} \\
   
   \includegraphics[width=0.8in]{n02606052_2969_real} &
   \includegraphics[width=0.8in]{n02606052_2969_gen_0} &
   \includegraphics[width=0.8in]{n02606052_2969_gen_1} \\
   
   \includegraphics[width=0.8in]{n01914609_1607_real} &
   \includegraphics[width=0.8in]{n01914609_1607_gen_0} &
   \includegraphics[width=0.8in]{n01914609_1607_gen_1} \\

   \includegraphics[width=0.8in]{n02607072_6241_real} &
   \includegraphics[width=0.8in]{n02607072_6241_gen_0} &
   \includegraphics[width=0.8in]{n02607072_6241_gen_1} \\

\end{tabular}
\caption{Samples from our Imagenet testing set. The network can both recover color and also correct color if a small amount is 
present.}
\label{fig:test_samples}
\end{figure}

% Directly from the first paper on the inception score: We find that itâ€™s important to evaluate the metric on a large
% enough number of samples (i.e. 50k) as part of this metric measures diversity.
% % Therefore we should not include it I think
% \footnote{Code for inception score can be found at https://github.com/openai/improved-gan}
%\begin{center}
%\begin{tabular}{c c c c c c}
%Samples & real & ugan & ugan-p & cyclegan \\ \hline
%Model & True Images & UGAN & UGAN-P & CycleGAN \\ \hline
%Score $\pm$ std. & $5.69 \pm 0.23$ & $6.18 \pm 0.35$ & $6.30 \pm 0.32$ & $6.37 \pm 0.43$ \\ \hline
%\end{tabular}
%\end{center}

\subsection{Comparison to CycleGAN}
It is important to note that during the process of learning a mapping $F: X \rightarrow Y$, CycleGAN also learns a
mapping $G: Y \rightarrow X$. Here we show that UGAN and UGAN-P provide superior performance. We use the Canny edge
detector \cite{canny1986computational} to provide a color agnostic evaluation of the images, as the original contain
distorted colors and cannot be compared back to as ground truth. Due to the fact that restoring color information
should not alter the overall structure of the image, we measure the distance in the image space between the edges
found in the original and generated images. Figure ~\ref{fig:canny_samples} shows the original images and results from
edge detection. Table 1 provides the measurements from Figure ~\ref{fig:canny_samples}, as well as the average over the
entire dataset.

We explore the artifacts of content loss, as seen in Figure ~\ref{fig:zoom}. In particular, we compare local statistics
of the highlighted image patches, where each image patch is resized to $64 \times 64$. We use the GDL
\cite{mathieu2015deep} from (\ref{gdl_eq}) as a sharpness measure. 

% I would like this table to match up with the figure showing edge detections. I wasn't able to
% get letters on the left side at each row though.
\begin{table}
\centering
\caption{Distances in image space}
\begin{tabular}{| c | c | c | c |}
   \hline
   Row/Method & CycleGAN & UGAN & UGAN-P \\ \hline
   A          & 116.45 & 85.71  & 86.15  \\ \hline
   B          & 114.49 & 97.92  & 101.01 \\ \hline
   C          & 120.84 & 96.53  & 97.57  \\ \hline
   D          & 129.27 & 108.90 & 110.50 \\ \hline
   Mean       & 111.60 & 94.91  & 96.51 \\ \hline
\end{tabular}
\end{table}

\newpage

\begin{figure}
\centering
\begin{tabular}{p{1.7cm} p{1.7cm} p{1.7cm} p{1.7cm}}
  
   ~\quad Original & ~CycleGAN & ~\quad UGAN & \quad UGAN-P \\

   \includegraphics[width=0.8in]{1_original} &
   \includegraphics[width=0.8in]{1_cimg} &
   \includegraphics[width=0.8in]{1_u0img} &
   \includegraphics[width=0.8in]{1_u1img} \\ [-1ex]
   \includegraphics[width=0.8in]{1_oedges} &
   \includegraphics[width=0.8in]{1_cedges} &
   \includegraphics[width=0.8in]{1_u0edges} &
   \includegraphics[width=0.8in]{1_u1edges} \\

   \includegraphics[width=0.8in]{2_original} &
   \includegraphics[width=0.8in]{2_cimg} &
   \includegraphics[width=0.8in]{2_u0img} &
   \includegraphics[width=0.8in]{2_u1img} \\ [-1ex]
   \includegraphics[width=0.8in]{2_oedges} &
   \includegraphics[width=0.8in]{2_cedges} &
   \includegraphics[width=0.8in]{2_u0edges} &
   \includegraphics[width=0.8in]{2_u1edges} \\

   \includegraphics[width=0.8in]{3_original} &
   \includegraphics[width=0.8in]{3_cimg} &
   \includegraphics[width=0.8in]{3_u0img} &
   \includegraphics[width=0.8in]{3_u1img} \\ [-1ex]
   \includegraphics[width=0.8in]{3_oedges} &
   \includegraphics[width=0.8in]{3_cedges} &
   \includegraphics[width=0.8in]{3_u0edges} &
   \includegraphics[width=0.8in]{3_u1edges} \\
   
   \includegraphics[width=0.8in]{4_original} &
   \includegraphics[width=0.8in]{4_cimg} &
   \includegraphics[width=0.8in]{4_u0img} &
   \includegraphics[width=0.8in]{4_u1img} \\ [-1ex]
   \includegraphics[width=0.8in]{4_oedges} &
   \includegraphics[width=0.8in]{4_cedges} &
   \includegraphics[width=0.8in]{4_u0edges} &
   \includegraphics[width=0.8in]{4_u1edges} \\

\end{tabular}
\caption{Running the Canny Edge Detector on sample images. Both variants of UGAN contain less noise than CycleGAN,
and are closer in the image space to the original. For each pair, the top row is the input image, and bottom row
the result of the edge detector.}
\label{fig:canny_samples}
\end{figure}

% TODO definitely need help with formatting, but content is there
\begin{figure*}
\centering
\begin{tabular}{p{4.0cm} p{4.0cm} p{4.0cm} p{4.0cm}}
  
   \qquad \qquad ~ Original & \qquad \qquad CycleGAN & \qquad \qquad ~ \textbf{UGAN} & \qquad \qquad \textbf{UGAN-P} \\

   \includegraphics[width=1.7in]{flickr_cmp} &
   \includegraphics[width=1.7in]{cgan_cmp} &
   \includegraphics[width=1.7in]{ugan_cmp} &
   \includegraphics[width=1.7in]{uganp_cmp} \\ [-11ex]
   
\end{tabular}
\end{figure*}

\begin{figure*}
\begin{tabular}{p{1.8cm} p{1.85cm} p{1.8cm} p{1.8cm} p{1.8cm} p{1.8cm} p{1.8cm} p{1.9cm} }
   
   \includegraphics[width=0.8in]{flickr_crop1} &
   \includegraphics[width=0.8in]{flickr_crop2} &
   \includegraphics[width=0.8in]{cgan_crop1} &
   \includegraphics[width=0.8in]{cgan_crop2} &
   \includegraphics[width=0.8in]{ugan_crop1} &
   \includegraphics[width=0.8in]{ugan_crop2} &
   \includegraphics[width=0.8in]{ugan_crop1} &
   \includegraphics[width=0.8in]{ugan_crop2} \\

   \includegraphics[width=0.8in]{flickr_crop3} &
   \includegraphics[width=0.8in]{flickr_crop4} &
   \includegraphics[width=0.8in]{cgan_crop3} &
   \includegraphics[width=0.8in]{cgan_crop4} &
   \includegraphics[width=0.8in]{ugan_crop3} &
   \includegraphics[width=0.8in]{ugan_crop4} &
   \includegraphics[width=0.8in]{ugan_crop3} &
   \includegraphics[width=0.8in]{ugan_crop4} \\


\end{tabular}
\caption{Comparison of local image patches. Each patch was resized to $64 \times 64$.}
\label{fig:zoom}
\end{figure*}


\subsection{Diver Tracking using Frequency-Domain Detection}
We investigate the frequency-domain characteristics of the restored images through a case-study of periodic motion tracking in 
sequence of images. Particularly, we compared the performance of Mixed Domain Periodic Motion (MDPM)- tracker 
\cite{islam2017mixed} on a sequence of images of a diver swimming in  arbitrary directions. MDPM tracker is designed for 
underwater robots to follow scuba divers by   tracking distinct frequency-domain signatures (high-amplitude spectra at $1$-$2$Hz) 
pertaining to human swimming. Amplitude spectra in frequency-domain correspond to the periodic intensity variations in image-space 
over time, which is often eroded in noisy underwater images \cite{shkurti2017underwater}.

Fig. \ref{mdpmStuff} illustrates the improved performance of MDPM tracker on generated images compared to the real ones. 
Underwater images often fail to capture the true contrast in intensity values between foreground and background due to low 
visibility. The generated images seem to restore these eroded intensity variations to some extent, causing much improved positive 
detection for MDPM tracker.

\begin{figure*}
\centering
\begin{tabular}{p{4.0cm} p{4.0cm} p{4.0cm} p{4.0cm}}
   \includegraphics[width=1.7in]{mdpm/real1} &
   \includegraphics[width=1.7in]{mdpm/real2} &
   \includegraphics[width=1.7in]{mdpm/real3} &
   \includegraphics[width=1.7in]{mdpm/real4} \\
   \includegraphics[width=1.7in]{mdpm/gen1} &
   \includegraphics[width=1.7in]{mdpm/gen2} &
   \includegraphics[width=1.7in]{mdpm/gen3} &
   \includegraphics[width=1.7in]{mdpm/gen4} \\
\end{tabular}
\label{fig:mdpm}
\vspace{4mm}

\begin{tabular}{l|c|c|c|r|}
 \cline{2-5}
 &  Correct detection & Wrong detection & Missed detection & Total \# of frames\\ \hline  \cline{2-4}
On real images  &  42 & 14 & 444 & 500  \\ \hline
On generated images  &  147 & 24 & 329 & 500  \\ \hline
\end{tabular}

\caption{Performance of MDPM tracker \cite{islam2017mixed} on both real (top row) and generated (second row) images; the Table 
compares the detection performance for both sets of images over a sequence of $500$ frames.   }
\label{mdpmStuff}
\end{figure*}


%Flippers of a human diver typically oscillate at frequencies between $1$ and $2$ Hz, which produces periodic intensity variations 
in the image-space over time. These variations correspond to distinct signatures in the frequency-domain (high-amplitude spectra 
at $1$-$2$Hz), which can be used for reliable detection.
