\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{shkurti2012multi}
\citation{whitcomb2000advances}
\citation{bingham2010robotic}
\citation{krizhevsky2012imagenet}
\citation{zhang2016colorful}
\citation{zhu2017unpaired}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\newlabel{fig:samples}{{I}{1}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Sample underwater images with natural and man-made artifacts (which in this case is our underwater robot) displaying the diversity of distortion that can occur. With the varying camera-to-object distances in the images, the distortion and loss of color varies between the different images.}}{1}{figure.1}}
\citation{zhang2016colorful}
\citation{iizuka2016let}
\citation{jordt2014underwater}
\citation{torres2005color}
\citation{li2017watergan}
\citation{pathak2016context}
\citation{Gatys_2016_CVPR}
\citation{isola2016image}
\citation{zhu2017unpaired}
\citation{isola2016image}
\citation{goodfellow2014generative}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\newlabel{sec:related}{{II}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Methodology}{2}{section.3}}
\newlabel{sec:methodology}{{III}{2}{Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-A}Dataset Generation}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-B}Adversarial Networks}{2}{subsection.3.2}}
\citation{mao2016least}
\citation{arjovsky2017wasserstein}
\citation{gulrajani2017improved}
\citation{zhao2016energy}
\citation{arjovsky2017wasserstein}
\citation{villani2008optimal}
\citation{gulrajani2017improved}
\citation{arjovsky2017wasserstein}
\citation{gulrajani2017improved}
\citation{mathieu2015deep}
\citation{isola2016image}
\citation{ronneberger2015u}
\citation{pmlr-v37-ioffe15}
\citation{nair2010rectified}
\citation{ulyanov2016instance}
\citation{radford2015unsupervised}
\citation{gulrajani2017improved}
\citation{ba2016layer}
\citation{isola2016image}
\citation{li2016precomputed}
\newlabel{fig:cgan_samples}{{III-B}{3}{Adversarial Networks}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Paired samples of ground truth and distorted images generated by CycleGAN. Top row: Ground truth. Bottom row: Generated samples.}}{3}{figure.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-C}Image Gradient Difference Loss}{3}{subsection.3.3}}
\newlabel{gdl_eq}{{5}{3}{Image Gradient Difference Loss}{equation.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-D}Network Architecture and Training Details}{3}{subsection.3.4}}
\citation{deng2009imagenet}
\citation{canny1986computational}
\citation{kingma2014adam}
\citation{abadi2016tensorflow}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experiments}{4}{section.4}}
\newlabel{sec:experiments}{{IV}{4}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Datasets}{4}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Evaluation}{4}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Samples from our Imagenet testing set. The network can both recover color and also correct color if a small amount is present.}}{4}{figure.3}}
\newlabel{fig:test_samples}{{3}{4}{Samples from our Imagenet testing set. The network can both recover color and also correct color if a small amount is present}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-C}Training and Inference Performance}{4}{subsection.4.3}}
\citation{canny1986computational}
\citation{mathieu2015deep}
\citation{islam2017mixed}
\citation{shkurti2017underwater}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Distances in image space}}{5}{table.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-D}Comparison to CycleGAN}{5}{subsection.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Running the Canny Edge Detector on sample images. Both variants of UGAN contain less noise than CycleGAN, and are closer in the image space to the original. For each pair, the top row is the input image, and bottom row the result of the edge detector.}}{5}{figure.4}}
\newlabel{fig:canny_samples}{{4}{5}{Running the Canny Edge Detector on sample images. Both variants of UGAN contain less noise than CycleGAN, and are closer in the image space to the original. For each pair, the top row is the input image, and bottom row the result of the edge detector}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-E}Diver Tracking using Frequency-Domain Detection}{5}{subsection.4.5}}
\citation{islam2017mixed}
\citation{islam2017mixed}
\bibdata{cambibs}
\bibcite{shkurti2012multi}{1}
\bibcite{whitcomb2000advances}{2}
\bibcite{bingham2010robotic}{3}
\bibcite{krizhevsky2012imagenet}{4}
\bibcite{zhang2016colorful}{5}
\bibcite{zhu2017unpaired}{6}
\bibcite{iizuka2016let}{7}
\bibcite{jordt2014underwater}{8}
\bibcite{torres2005color}{9}
\bibcite{li2017watergan}{10}
\bibcite{pathak2016context}{11}
\bibcite{Gatys_2016_CVPR}{12}
\bibcite{isola2016image}{13}
\bibcite{goodfellow2014generative}{14}
\bibcite{mao2016least}{15}
\bibcite{arjovsky2017wasserstein}{16}
\bibcite{gulrajani2017improved}{17}
\bibcite{zhao2016energy}{18}
\bibcite{villani2008optimal}{19}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of local image patches. Each patch was resized to $64 \times 64$.}}{6}{figure.5}}
\newlabel{fig:zoom}{{5}{6}{Comparison of local image patches. Each patch was resized to $64 \times 64$}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{6}{section.5}}
\@writefile{toc}{\contentsline {section}{References}{6}{section*.2}}
\bibcite{mathieu2015deep}{20}
\bibcite{ronneberger2015u}{21}
\bibcite{pmlr-v37-ioffe15}{22}
\bibcite{nair2010rectified}{23}
\bibcite{ulyanov2016instance}{24}
\bibcite{radford2015unsupervised}{25}
\bibcite{ba2016layer}{26}
\bibcite{li2016precomputed}{27}
\bibcite{deng2009imagenet}{28}
\bibcite{canny1986computational}{29}
\bibcite{kingma2014adam}{30}
\bibcite{abadi2016tensorflow}{31}
\bibcite{islam2017mixed}{32}
\bibcite{shkurti2017underwater}{33}
\bibstyle{ieeetr}
\newlabel{fig:mdpm}{{IV-E}{7}{Diver Tracking using Frequency-Domain Detection}{subsection.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Performance of MDPM tracker \cite  {islam2017mixed} on both real (top row) and generated (second row) images; the Table compares the detection performance for both sets of images over a sequence of $500$ frames. }}{7}{figure.6}}
\newlabel{mdpmStuff}{{6}{7}{Performance of MDPM tracker \cite {islam2017mixed} on both real (top row) and generated (second row) images; the Table compares the detection performance for both sets of images over a sequence of $500$ frames}{figure.6}{}}
